<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight: 300;
		font-size: 18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}

	h1 {
		font-size: 32px;
		font-weight: 300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	a:link,
	a:visited {
		color: #1367a7;
		text-decoration: none;
	}

	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35),
			/* The top layer shadow */
			5px 5px 0 0px #fff,
			/* The second layer */
			5px 5px 1px 1px rgba(0, 0, 0, 0.35),
			/* The second layer shadow */
			10px 10px 0 0px #fff,
			/* The third layer */
			10px 10px 1px 1px rgba(0, 0, 0, 0.35),
			/* The third layer shadow */
			15px 15px 0 0px #fff,
			/* The fourth layer */
			15px 15px 1px 1px rgba(0, 0, 0, 0.35),
			/* The fourth layer shadow */
			20px 20px 0 0px #fff,
			/* The fifth layer */
			20px 20px 1px 1px rgba(0, 0, 0, 0.35),
			/* The fifth layer shadow */
			25px 25px 0 0px #fff,
			/* The fifth layer */
			25px 25px 1px 1px rgba(0, 0, 0, 0.35);
		/* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35);
		/* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35),
			/* The top layer shadow */
			5px 5px 0 0px #fff,
			/* The second layer */
			5px 5px 1px 1px rgba(0, 0, 0, 0.35),
			/* The second layer shadow */
			10px 10px 0 0px #fff,
			/* The third layer */
			10px 10px 1px 1px rgba(0, 0, 0, 0.35);
		/* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}

	hr {
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>

<head>
	<title>Test-Time Adaptation for Visual Document Understanding
, , 

</title>
	<meta property="og:image" content="resources/model.png" />
	<!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title"
		content="Learning and Verification of Task Structure in Instructional Videos" />
	<meta property="og:description"
		content="We introduce an approach for summarizing instrucitonal videos without relying on manual annotations." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<!-- <script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script> -->
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Test-Time Adaptation for Visual Document Understanding</span>
		<br><br>
		<table align=center width=900px>
			<table align=center width=1100px>
				<tr>
					<td align=center width=350px>
						<span style="font-size:25px"><a href="https://saynaebrahimi.github.io/">Sayna Ebrahimi,</a></span>&nbsp;&nbsp;&nbsp;
						<span style="font-size:25px"><a href="https://sites.google.com/corp/view/sercanarik/home">Sercan O. Arik,</a></span>&nbsp;&nbsp;&nbsp;
						<span style="font-size:25px"><a href="https://tomas.pfister.fi/">Tomas Pfister</a></span>&nbsp;&nbsp;&nbsp;
					</td>
				</tr>
			</table>
			<br>
			<table align=center width=500px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:23px">Google Cloud AI Research</span>&nbsp;&nbsp;&nbsp;
						</center>
					</td>
				</tr>
			</table>
			<br>
			<span style="font-size:27px">TMLR June 2023</span>
			<br><br>
			<table align=center width=350px>
				<tr>
					<td align=center width=100px> <span style="font-size:15pt">
							<center>
								<a href="https://arxiv.org/pdf/2206.07240.pdf">[Paper]</a>

							</center>
					</td>
					<td align=center width=100px> <span style="font-size:15pt">
							<center>
								<a href="https://openreview.net/pdf?id=zshemTAa6U">[Bibtex]</a>
							</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<br>
<!-- 	<center>
		<p align="center">
			<iframe width="760" height="480" src="" frameborder="0"
					allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
		</p>
	</center>
 -->
<!--  -->

<body>
	<center>
  <div class="video-container">
	<iframe src="./index_files/doctta/DocTTA_TMLR.mp4" width="640" height="480" controlList="nodownload" frameborder="0" allowfullscreen align="center"></iframe>
  </div>
	</center>
</body>

	<hr>
	<center>
		<table align=center width=850px>
			<center>
				<h1>Abstract</h1>
			</center>
			<tr>
				<td>
					Self-supervised pretraining has been able to produce transferable representations for various visual document understanding (VDU) tasks. However, the ability of such representations to adapt to new distribution shifts at test-time has not been studied yet. We propose DocTTA, a novel test-time adaptation approach for documents that leverages cross-modality self-supervised learning via masked visual language modeling as well as pseudo labeling to adapt models learned on a source domain to an unlabeled target domain at test time. We also introduce new benchmarks using existing public datasets for various VDU tasks including entity recognition, key-value extraction, and document visual question answering tasks where DocTTA improves the source model performance up to 1.79% in (F1 score), 3.43% (F1 score), and 17.68% (ANLS score), respectively while drastically reducing calibration error on target data. 
				</td>
			</tr>
		</table>
	</center>
	<br>







	<hr>
	<center>
		<table align=center width=850px>
			<center>
				<h1>Benchmark Datasets</h1>
			</center>
			<tr>
				<td>
					<br>
					<br>
					To better highlight the impact of distribution shifts and to study the methods that are robust against them, we introduce new benchmarks for VDU. Our benchmark datasets are constructed from existing popular and publicly-available VDU data to mimic real-world challenges.

				<head>
  <style>
    .container {
      display: flex;
      align-items: flex-start;
    }
    .column {
      width: 33.33%;
      padding: 10px;
    }
    .header {
      font-weight: bold;
    }
  </style>
				</head>
				<body>
				  <div class="container">
				    <div class="column">
				      <h2 class="header"><a href="https://drive.google.com/drive/u/1/folders/1Sxpxsl-KU9x8Dj6VTJMpOazIwXuDc4dh">FUNSD-TTA</a> </h2>
				      <p>We consider <a href="https://guillaumejaume.github.io/FUNSD/">FUNSD</a> dataset for this benchmark which is a noisy form understanding collection with 9707 semantic entities and 31,485 words with 4 categories of entities question, answer, header, and other, where each category (except other) is either the beginning or the intermediate word of a sentence. Therefore, in total, we have 7 class labels. We first combine the original training and test splits and then manually divide them into two groups. We set aside 149 forms that are filled with more texts for the source domain and put 50 forms that are sparsely filled for the target domain. We randomly choose 10 out of 149 documents for validation, and the remaining 139 for training.</p>
				    </div>
				    
				    <div class="column">
				      <h2 class="header"><a href="https://drive.google.com/drive/u/1/folders/1nJGjugGOweWlmkRu1nGhpA0kXQ_iaKO4">SROIE-TTA</a></h2>
				      <p>We use <a href="https://github.com/zzzDavid/ICDAR-2019-SROIE">SROIE</a> dataset with 9 classes in total. Similar to FUNSD, we first combine the original training and test splits. Then, we manually divide them into two groups based on their visual appearance â€“ source domain with 600 documents contains standard-looking receipts with proper angle of view and clear black ink color. We use 37 documents from this split for validation, which we use to tune adaptation hyperparameters. Note that the validation split does not overlap with the target domain which has 347 receipts with slightly blurry look, rotated view, colored ink, and large empty margins.</p>
				    </div>
				    
				    <div class="column">
				      <h2 class="header"><a href="https://drive.google.com/drive/u/1/folders/1ZQuGKUVK73X5XD5mr1vs3CI9K2NA-vQj">DocVQA-TTA</a></h2>
				      <p>We use <a href="https://www.docvqa.org/">DocVQA</a>, a large-scale VQA dataset with nearly 20 different types of documents including scientific reports, letters, notes, invoices, publications, tables, etc. The original training and validation splits contain questions from all of these document types. However, for the purpose of creating an adaptation benchmark, we select 4 domains of documents: i) Emails & Letters (L), ii) Tables & Lists (T), iii) Figure & Diagrams (F), and iv) Layout (L).</p>
				    </div>
				  </div>
				</body>

				</td>
			</tr>
		</table>
	</center>
	<br>



	<table align=center width=700 px>
		<center>
			<h1>Paper</h1>
		</center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./index_files/doctta/doctta_paper.jpg" /></a></td>
			<td><span style="font-size:12pt">S. Ebrahimi, S. Arik, T. Pfister</span><br>
				<b><span style="font-size:12pt">Test-Time Adaptation for Visual Document Understanding</span></b><br>
				<span style="font-size:12pt">TMLR, 2023.</span>
			</td>
			</td>
		</tr>
	</table>
	<br>
	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt">
					<center>
						<a href="https://arxiv.org/pdf/2206.07240.pdf">[Paper]</a> |
						<a href="https://arxiv.org/abs/2206.07240">[Bibtex]</a>
					</center>
			</td>
		</tr>
	</table>
	<br>
	<hr>

	<br>



	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
		<tr>
			<td>
				<br>
				<p align="right">
					<font size="2">
						<a href="https://github.com/richzhang/webpage-template">Template cloned from here!</a>
					</font>
				</p>
			</td>
		</tr>
	</table>


	<br>
</body>

</html>